---
layout:     post
title:      Unsupervised learning for representing the meaning of text
date:       2018-06-10 8:00:00
author:     Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
visible:    false
---

##Understanding what our embeddings encode using compressed sensing

Re-expression of DisC as matrix transform of BonG.
Compressed sensing implies recovery of BonG when A is RIP.
Relevant to classification (compressed learning). State theorem.

A is RIP for Rademacher vectors (BOS, see paper for full proof).
State main theorem (for DisC, not LSTM).
Mention connection to LSTM.

<div style="text-align:center;">
<img src="/assets/imdbperf_uni_bi.svg" style="width:400px;" />
</div>

But what about pretrained word embeddings?
