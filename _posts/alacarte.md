---
layout:     post
title:      Simple and efficient semantic embeddings for unseen words and features
date:       2018-09-01 10:00:00
author:     Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
visible:    False
---

In this blog we broaden our recent discussion of deep-learning-free text embeddings (see our [previous post](http://www.offconvex.org/2018/06/25/textembeddings/)) to include simple but principled embeddings for arbitrary language features.
As with modern semantic word vectors, our method is rooted in Firth's [distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), "you shall know a word by the company it keeps."
Under this view each word's embedding can be seen as a concise representation of the distribution of *other* words occurring in the same context (see also [Sanjeev's post](http://www.offconvex.org/2015/12/12/word-embeddings-1/) on the topic). 
By making a shallow extension to this hypothesis, e.g. "you shall know a bigram/sense/entity by the company it keeps," our *à la carte* approach allows us to pick out exactly those features required by a downstream task.
We can then embed them using only the pretrained vectors of words that occur next to them in a large text corpus plus a linear regression step.

Despite this simplicity, *à la carte* embedding leads to state-of-the-art results on several document classsification tasks and for one-shot learning of word vectors.
These results appear in our [ACL'18 paper](http://aclweb.org/anthology/P18-1002) with Yingyu Liang, Tengyu Ma, and Brandon Stewart.
We will also discuss Sanjeev, Yingyu, and Tengyu's [TACL'18 paper](https://transacl.org/ojs/index.php/tacl/article/view/1346) with Yuanzhi Li and Andrej Risteski, which provides the theoretical motivation for our approach.

## Mathematical background: relating word embeddings and their contexts

We start by assuming a large text corpus $C$, over which we have trained high-quality word embeddings $v_w$. 
Then for some text feature $f$ (such as a bigram) we want to use these embeddings, together with the contexts $C_f$ that $f$ appears in, to get an embedding $v_f$.
This simple setup is illustrated below.

<p style="text-align:center;">
<img src="/assets/alacarte_problem.svg" width="50%"  alt="Problem Setup" />
</p>

The simplest thing to do here would be to represent $f$ as the average of all the words in its contexts, i.e. 

$$ v_f^\textrm{avg} = \frac{1}{|C_f|}\sum\limits_{\textrm{context }c\in C_f}\frac{1}{|c|}\sum\limits_{\textrm{word }w\in c}v_w $$

This approach fits nicely with objectives such as skip-gram, in which word embeddings are trained to be close to the sum of the context word embeddings.
However, taking the average over the entire corpus will amplify a few common components at the expense of others, both due to the many stops words and because word-embeddings are known to share a few common directions.
Some ways to alleviate this problem include taking out the top (or top few) singular components or removing stopwords (or otherwise down-weighting frequent words).

A natural extension of such approaches is to *learn* which components to remove, i.e. to find a linear transform $A^\ast$ of $v_f^\textrm{additive}$ that is optimal in some sense.
Since we already have high-quality word embeddings for the corpus, it is natural to use them as targets, i.e. to solve the linear regression

$$ A^\ast=\arg\min\sum\limits_w\|v_w-Av_w^\textrm{avg}\|_^2 $$

Then the embedding of a feature $f$ can be simply set to $v_f=A^\astv_f^\textrm{avg}$.

Should we expect there to be a linear  transformation between embeddings of words $v_w$ and average embeddings of *contexts* of words $v_w^\textrm{avg}$?
For GloVe-like embeddings, the answer turns out to be yes (empirically, our method works for word2vec as well).
Consider a latent-variable model of corpus generation, in which each window $c$ of $n$ words is generated by drawing a context vector $x\sim\mathcal{N}(0_d,\Sigma)$ and then sampling words $w_1,\dots,w_n$ with $\mathbb{P}(w_i)\propto\exp\langle x,v_{w_i}\rangle$.
Under this modification of the rand-walk model, whose approximate MLE objective is similar to that of GloVe, we have the following theorem: 

$$ \exists~A\in\mathbb{R}^{d\times d}\textrm{ s.t. }v_w=A\mathbb{E}_{c\sim C}\left[\frac{1}{n}\sum\limits_{w'\in c}v_{w'}\bigg|w\in c\right]~\forall~w $$

This says that there exists some linear transform mapping the expected average context vector of each word to that word's word embedding.
The linear regression above returns the MLE for $A$ under this model, and in experiments the average cosine similarity between $v_w$ and $Av_f^\textrm{avg}$ is 0.9 or higher.

This result also explains the linear algebraic structure of the embeddings of polysemous words (words having multiple possible meanings, such as *tie*) discussed in an earlier [post](http://www.offconvex.org/2016/07/10/embeddingspolysemy/).
Assuming for simplicity that $tie$ only has two meanings (*clothing* and *game*), it is easy to see that its word embedding is a linear transformation of the sum of the average context vectors of its two senses:

$$ v_w=A\mathbb{E}v_w^\textrm{avg}=A\mathbb{E}\left[v_\textrm{clothing}^\textrm{avg}+v_\textrm{game}^\textrm{avg}\right]=A\mathbb{E}v_\textrm{clothing}^\textrm{avg}+A\mathbb{E}v_\textrm{game}^\textrm{avg} $$

This equation also shows that we can get a reasonable estimate for the vector of the sense *clothing* (and, by extension many other features of interest) by setting $v_\textrm{clothing}=A^\ast v_\textrm{clothing}^\textrm{avg}$.

## One-shot and few-shot induction of embedding for rare words

## A la carte embeddings of $n$-grams and their application to simple sentence representations

## Discussion
