---
layout:     post
title:      Simple and efficient semantic embeddings for any language feature
date:       2018-09-01 10:00:00
author:     Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
visible:    False
---

In this blog we broaden our recent discussion of deep-learning-free text embeddings (see our [previous post](http://www.offconvex.org/2018/06/25/textembeddings/)) to include simple but principled embeddings for arbitrary language features.
As with modern semantic word vectors, our method is rooted in Firth's [distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), "you shall know a word by the company it keeps."
Under this view each word's embedding can be seen as a concise representation of the distribution of *other* words occurring in the same context (see also [Sanjeev's post](http://www.offconvex.org/2015/12/12/word-embeddings-1/) on the topic). 
By making a shallow extension of this hypothesis, e.g. "you shall know a bigram/sense/entity by the company it keeps," our *à la carte* approach allows us to pick out exactly those features required by a downstream task and embed only them.
We can then embed them using only the pretrained vectors of words that occur next to them in a large text corpus plus a linear regression step.

Despite this simplicity, *à la carte* embedding leads to state-of-the-art results on several document classsification tasks and for one-shot learning of word embeddings.
These results appear in our [ACL'18 paper](http://aclweb.org/anthology/P18-1002) with Yingyu Liang, Tengyu Ma, and Brandon Stewart.
The discussion will also highlight Sanjeev, Yingyu, and Tengyu's [TACL'18 paper](https://transacl.org/ojs/index.php/tacl/article/view/1346) with Yuanzhi Li and Andrej Risteski, which provides the theoretical motivation for our approach.

## Mathematical background: relating word embeddings and their contexts

We start by assuming a large text corpus $C$, over which we have trained high-quality word embeddings $v_w$. 
Then for some text feature $f$ (such as a bigram) we want to use these embeddings, together with the contexts $C_f$ that $f$ appears in, to get an embedding $v_f$.
This simple setup is illustrated below.

<p style="text-align:center;">
<img src="/assets/alacarte_problem.svg" width="50%"  alt="Problem Setup" />
</p>

The simplest thing to do here would be to represent $f$ as the average of all the words in its contexts, i.e. 

$$ v_f^\textrm{avg} = \frac{1}{|C_f|}\sum\limits_{\textrm{context }c\in C_f}\frac{1}{|c|}\sum\limits_{\textrm{word }w\in c}v_w $$

This approach fits nicely with objectives such as skip-gram, in which word embeddings are trained to be close to the sum of the context word embeddings.
However, taking the average over the entire corpus will amplify a few common components at the expense of others, both due to the many stops words and because word-embeddings are known to share a few common directions.
Some ways to alleviate this problem include taking out the top (or top few) singular components or removing stopwords (or otherwise down-weighting frequent words).

A natural extension of such approaches is to *learn* which components to remove, i.e. to find a linear transform $A^\ast$ of $v_f^\textrm{avg}$ that is optimal in some sense.
Since we already have high-quality word embeddings for the corpus, we can use them to learn the matrix $A^\ast$ by solving the linear regression

$$ A^\ast=\arg\min\sum\limits_w\|v_w-Av_w^\textrm{avg}\|_2^2 $$

Then the embedding of a feature $f$ can be simply set to $v_f=A^\ast v_f^\textrm{avg}$.

Should we expect there to be a linear transformation between embeddings of words $v_w$ and average embeddings of *contexts* of words $v_w^\textrm{avg}$?
For GloVe-like embeddings, the answer turns out to be *yes* (and empirically for word2vec as well).
Consider a latent-variable model of corpus generation, in which each window $c$ of $n$ words is generated by drawing a context vector $x\sim\mathcal{N}(0_d,\Sigma)$ and then sampling words $w_1,\dots,w_n$ with $\mathbb{P}(w_i)\propto\exp\langle x,v_{w_i}\rangle$.
Under this modification of the rand-walk model, whose approximate MLE objective is similar to that of GloVe, we have the following theorem: 

$$ \exists~A\in\mathbb{R}^{d\times d}\textrm{ s.t. }v_w=A\mathbb{E} \left[\frac{1}{n}\sum\limits_{w'\in c}v_{w'}\bigg|w\in c\right]=A\mathbb{E}v_w^\textrm{avg}~\forall~w $$

where the expectation is taken over possible contexts $c$. This says that there exists some linear transform mapping the expected average context vector of each word to that word's word embedding.
The linear regression above returns the MLE for $A$ under this model, and in experiments the average cosine similarity between $v_w$ and $Av_f^\textrm{avg}$ is 0.9 or higher.

This result also explains the linear algebraic structure of the embeddings of polysemous words (words having multiple possible meanings, such as *tie*) discussed in an earlier [post](http://www.offconvex.org/2016/07/10/embeddingspolysemy/).
Assuming for simplicity that $tie$ only has two meanings (*clothing* and *game*), it is easy to see that its word embedding is a linear transformation of the sum of the average context vectors of its two senses:

$$ v_w=A\mathbb{E}v_w^\textrm{avg}=A\mathbb{E}\left[v_\textrm{clothing}^\textrm{avg}+v_\textrm{game}^\textrm{avg}\right]=A\mathbb{E}v_\textrm{clothing}^\textrm{avg}+A\mathbb{E}v_\textrm{game}^\textrm{avg} $$

This equation also shows that we can get a reasonable estimate for the vector of the sense *clothing* (and, by extension many other features of interest) by setting $v_\textrm{clothing}=A^\ast v_\textrm{clothing}^\textrm{avg}$.

## One-shot and few-shot induction of embedding for rare words

We put the theory to test by checking how well we can induce embeddings for words with just one or a few occurrences in context.
The performance of standard word embedding methods is known to degrade in such low frequency settings.
In order to analyze the effect of number of contexts on the quality of induced embeddings we created the *[Contextual Rare Words](http://nlp.cs.princeton.edu/CRW/)* dataset (a subset of the [Rare Words](https://nlp.stanford.edu/~lmthang/morphoNLM/) dataset) where, along with word pairs and human-rated scores, we also provide contexts for the rare words.
We compare the performance of our method with the alternatives mentioned above and find that *à la carte* embedding consistently outperforms other methods and requires far fewer contexts to match their best performance.

<p style="text-align:center;">
<img src="/assets/crwplot.svg" width="40%" />
</p>

Additionally we evaluate our method on tasks that involve finding reliable embeddings for unseen words and concepts given a single definition or a few sentences of usage for these concepts.
To "simulate the process by which a competent speaker encounters a new word in known contexts," [Herbelot and Baroni](http://aclweb.org/anthology/D17-1030) constructed a "nonce" dataset consisting of single-word concepts and their definitions.
By replacing this competent speaker with a word embedding algorithm, the authors proposed an evaluation in which the embedding it produces using the definition is compared to a ground truth embedding obtained by full-corpus training.
As shown in the results below, the embedding *à la carte* induces using the definition is much closer to this true embedding than that produced by other methods, including a modification to word2vec developed by Herbelot and Baroni.

<p style="text-align:center;">
<img src="/assets/nonce.svg" width="40%" />
</p>

These experiments show that *à la carte* is able to induce high-quality word embeddings from very few uses in context, outperforming both simple methods like SIF and all-but-the-top and more sophisticated methods like word2vec and its modifications.

## A la carte embeddings of $n$-grams and their application to simple sentence representations

While the *à la carte* method for rare *word* embedding induction has theoretical justifications, one could also use it to induce embeddings for other kinds of rare *features*.
We test this hypothesis by inducing embeddings for $n$-grams by using contexts from a large text corpus and word embeddings trained on the same corpus.
A qualitative evaluation of the $n$-gram embeddings is done by finding the closest words to it in terms of cosine similarity between the embeddings.
As evident from the below figure, *à la carte* bigram embeddings capture the meaning of the phrase better than some other compositional and learned bigram embeddings.

<p style="text-align:center;">
<img src="/assets/ngram_quality.png" width="65%" />
</p>

We also use these $n$-gram embeddings to construct sentence embeddings, similarly to [DisC embeddings](http://www.offconvex.org/2018/06/25/textembeddings/), to evaluate on classification tasks.
A sentence is embedded as the concatenation of sums of embeddings for $n$-gram in the sentence for use in downstream classification tasks.
Using this simple approach we can match the performance of other linear and LSTM representations, even obtaining state-of-the-art results on some of them.

<p style="text-align:center;">
<img src="/assets/ngram_clf.png" width="80%" />
</p>

## Discussion

Apart from its simplicity and computational efficiency, the versatility of *à la carte* method allows us to induce embeddings for all kinds of linguistic features, as long as there is at least one usage of the feature in a context of words.
As the name promises, this method avoids the *prix fixe* cost of having to learn embeddings for all features at the same time and lets us embed only features of interest.
Despite this, our method is competitive with many other feature embedding methods and also beats them in many cases.
One issue that still needs tackling, however, is zero-shot learning of word embeddings i.e. inducing embeddings for a words/features without any context.
Compositional methods, including those using character level information such as [fastText](https://fasttext.cc/), have been successful at zero-shot learning of word embeddings and incorporating these ideas into the *à la carte* approach might be worth looking into.

We have made [available](https://github.com/NLPrinceton/ALaCarte) code for applying *à la carte* embedding, including to re-create the results described.
